'opel 1900', NA, 4L, 2123L, NA, 'European',
'peugeot 304', 30.0, 4L, 2074L, 19.5, 'European',
'volkswagen model 111', 27.0, 4L, 1834L, NA, 'European',
NA, 35.0, 4L, 1613L, 18.0, 'Japanese',
'datsun pl510', NA, 4L, 2130L, 14.5, 'Japanese'
)
autos
autos
autos %>%
filter(Origin == 'European' & MPG > 25) %>%
select('Model', 'Weight') %>%
arrange(desc(Weight))
select(arrange(filter(autos, MPG > 25), desc(Weight)), Model, Weight)
autos
autos %>%
filter(Origin == 'European' & MPG > 25) %>%
select(Model, Weight) %>%
arrange(desc(Weight))
autos %>%
filter(Origin == 'European') %>%
select(Model, Weight) %>%
arrange(desc(Weight))
autos %>%
filter(Origin == 'European') %>%
select(Model, Weight) %>%
arrange(desc(Weight))
autos %>%
filter(Origin == 'European' & MPG > 25) %>%
select(Model, Weight) %>%
arrange(desc(Weight))
rm(list = ls())
as.matrix(iris[ ,-5])
cnames <- c(
"Dewitt County",
"Lac qui parle County",
"St. John the Baptist Parish",
"Stone County",
"Lewis & Clark County"
)
cnames
# tolower
tolower(cnames)
# tolower
cn2 <- tolower(cnames)
cn2
library(stringr)
# tolower
cn2 <- str_to_lower(cn2)
cn2
str_sub(cn2, start = -7, end = -1)
str_sub(cn2, start = -1, end = -7)
# substring by positions
str_sub(cn2, start = 1, end = 7)
str_sub(cn2, start = -7, end = -1)
str_sub(cn2, start = -6, end = -1)
str_sub(cn2, start = 1, end = str_count(cn2))
str_sub(cn2, start = 1, end = str_count(cn2) - 6)
str_sub(cn2, start = 1, end = str_count(cn2) - 7)
# count
str_count(cn2)
# count (number of chars)
str_count(cn2)
# substring by positions
str_sub(cn2, start = 1, end = 7)
str_sub(cn2, start = -6, end = -1)
str_sub(cn2, start = 1, end = str_count(cn2) - 7)
str_split(cn3, pattern = " ")
cn3 <- str_sub(cn2, start = 1, end = str_count(cn2) - 7)
str_split(cn3, pattern = " ")
str_split(cn3, pattern = "")
# split by character
splits <- str_split(cn3, pattern = "")
splits
# look for '.' and '&'
splits[[3]]
splits[[3]] == '.'
splits[[3]][splits[[3]] != '.']
s3 <- splits[[3]][splits[[3]] != '.']
str_c(s3, collapse = '')
# eliminate ampersand
splits[[5]]
splits[[5]] == '&'
# eliminate ampersand
splits[[5]]
splits[[5]] == '&'
s5 <- splits[[5]][splits[[5]] != '&']
str_c(s5, collapse = '')
str_c(s5, collapse = '')
str_c(s3, collapse = '')
# eliminate ampersand
splits[[5]]
splits[[5]]
str_replace(splits[[5]], "&", "and")
s5 <- str_replace(splits[[5]], "&", "and")
str_c(s5, collapse = '')
dot
splits[[5]] == '&'
splits[[5]]
s5 <- splits[[5]]
which_amp <- (splits[[5]] == '&')
s5[which_amp] <- "and"
s5
str_c(s5, collapse = '')
str_c(s5, collapse = '')
splits[[5]] <- str_c(s5, collapse = '')
splits
str_c(s3, collapse = '')
s5
str_c(s5, collapse = '')
rm(list = ls())
# some strings
cnames <- c(
"Dewitt County",
"Lac qui parle County",
"St. John the Baptist Parish",
"Stone County",
"Lewis & Clark County"
)
# tolower
cn2 <- str_to_lower(cn2)
cn2
# tolower
cn2 <- str_to_lower(cnames)
cn2
# count (number of chars)
str_count(cn2)
# substring by positions
str_sub(cn2, start = 1, end = 7)
str_sub(cn2, start = -6, end = -1)
str_sub(cn2, start = 1, end = str_count(cn2) - 7)
cn3 <- str_sub(cn2, start = 1, end = str_count(cn2) - 7)
cn3
# split by character
splits <- str_split(cn3, pattern = "")
splits
# look for '.' and '&'
splits[[3]]
splits[[3]] == '.'
# look for '.' and '&'
splits[[3]]
which_dot <- (splits[[3]] == '.')
# eliminate dot
s3 <- splits[[3]][!which_dot]
s3
str_c(s3, collapse = '')
rm(list = ls())
ls()
setwd('~')
library(stringr)
# some strings
cnames <- c(
"Dewitt County",
"Lac qui parle County",
"St. John the Baptist Parish",
"Stone County",
"Lewis & Clark County"
)
cnames
# to lower
str_to_lower(cnames)
# to lower
cn2 <- str_to_lower(cnames)
cn2
str_to_title(cn2)
cn2
cn2
nchar(cn2)
str_count(cn2) # stringr
# splits
str_sub(cn2, start = 1, end = 5)
str_sub(cn2, end = -5)
cn2
str_sub(cn2, start = -6, end = -1)
str_sub(cn2, start = 1, end = str_count(cn2) - 6)
str_sub(cn2, start = 1, end = str_count(cn2) - 7)
str_locate(cn2, " ")
cn2
str_locate_all(cn2, " ")
str_sub(cn2, start = 1, end = str_count(cn2) - 7)
str_sub(cn2, start = 1, end = str_count(cn2) - 7)
cn3 <- str_sub(cn2, start = 1, end = str_count(cn2) - 7)
cn3
cn3
str_split(cn3, pattern = " ")
str_split(cn3, pattern = "")
cn4 <- str_split(cn3, pattern = "")
cn4
# the dot
cn4[[3]]
# the dot
cn4[[3]] == '.'
# the dot
cn4[cn4[[3]] != '.']
# the dot
cn4[[4]][cn4[[3]] != '.']
# the dot
cn4[[3]][cn4[[3]] != '.']
# the dot
cn4[[3]][cn4[[3]] != '.']
cn4[[3]][!(cn4[[3]] == '.')]
cn4[[5]]
# the &
cn4[[5]] == '&'
cn4[[5]][cn4[[5]] != '&']
cn4[[5]][!(cn4[[5]] == '&')]
s5 <- cn4[[5]]
s5
s5
s5 == "&"
s5[s5 == "&"]
s5[s5 == "&"] <- "and"
s5
s5
str_c(s5, collapse = '')
cn4
cn4
?str_c
lapply(cn4, str_c)
lapply(cn4, str_c(collapse = ""))
lapply(cn4, function(x) str_c(x, collapse = ""))
sapply(cn4, function(x) str_c(x, collapse = ""))
lapply(cn4, function(x) str_c(x, collapse = ""))
unlist(lapply(cn4, function(x) str_c(x, collapse = "")))
names(iris)[-5]
X <- as.matrix(iris[ ,-5])
y <- as.factor(iris$Species)
k <- nlevels(y)
n <- nrow(X)
nk <- as.vector(table(y))
# matrix of group means (i.e. centroids)
centroids <- matrix(0, ncol(X), k)
for (j in 1:ncol(X)) {
centroids[j,] <- tapply(X[,j], y, FUN=mean)
}
dimnames(centroids) <- list(colnames(X), levels(y))
centroids
# decomposing between-class matrix:  B = CC'
gm <- colMeans(X)
centroids_centered <- sweep(centroids, 1, gm, FUN="-")
C <- sweep(centroids_centered, 2, sqrt(nk/n), FUN="*")
C
between_variance <- function(X, y) {
X <- as.matrix(X)
y <- as.factor(y)
K <- nlevels(y)
levs <- levels(y)
nk <- as.vector(table(y))
g <- colMeans(X)
B <- 0
for (k in 1:K) {
gk <- colMeans(X[y == levs[k], ])
B <-  B + (nk[k]) * (gk - g) %*% t(gk - g)
}
rownames(B) <- colnames(X)
(1/(nrow(X)-1)) * B
}
## @knitr between_variance_test
# test between_variance()
between_variance(iris[ ,1:4], iris$Species)
## @knitr within_variance
# within variance
within_variance <- function(X, y) {
X <- as.matrix(X)
y <- as.factor(y)
K <- nlevels(y)
levs <- levels(y)
nk <- as.vector(table(y))
g <- colMeans(X)
W <- 0
for (k in 1:K) {
Xk <- scale(X[y == levs[k], ], scale = FALSE)
W <- W + t(Xk) %*% Xk
}
(1/(nrow(X)-1)) * W
}
## @knitr within_variance_test
# test within_variance()
within_variance(iris[ ,1:4], iris$Species)
## @knitr within_variance
# within variance
within_variance <- function(X, y) {
X <- as.matrix(X)
y <- as.factor(y)
K <- nlevels(y)
levs <- levels(y)
nk <- as.vector(table(y))
g <- colMeans(X)
W <- 0
for (k in 1:K) {
Xk <- scale(X[y == levs[k], ], scale = FALSE)
W <- W + t(Xk) %*% Xk
}
(1/(nrow(X)-1)) * W
}
## @knitr between_variance_test
# test between_variance()
between_variance(iris[ ,1:4], iris$Species)
## @knitr between_variance_test
# test between_variance()
between_variance(iris[ ,1:4], iris$Species)
C
c %*% t(C)
## @knitr between_variance_test
# test between_variance()
between_variance(iris[ ,1:4], iris$Species)
C %*% t(C)
C <- sweep(centroids_centered, 2, sqrt(nk/(n-1)), FUN="*")
C
## @knitr between_variance_test
# test between_variance()
between_variance(iris[ ,1:4], iris$Species)
C %*% t(C)
## @knitr W_matrix
# within-groups covariance matrix
W <- within_variance(X, y)
## @knitr eigen_decomposition
# eigen-decomposition
EIG <- eigen(t(C) %*% solve(W) %*% C)
lam <- EIG$values[1:(k - 1)]
U <- solve(W) %*% C %*% EIG$vectors[,1:(k-1)]
head(U)
library(MASS)
lda_fit <- lda(iris[ ,1:4], iris$Species)
names(lda_fit)
lda_fit$svd
lam
?lda
lda_fit$scaling
head(U)
U
cor(lda_fit$scaling, U)
cor(lda_fit$scaling[,1], U[,1])
plot(lda_fit$scaling[,1], U[,1])
cor(U)
EIG$vectors[,1:(k-1)]
plot(U)
plot(X %*% U)
library(ggplot2)
## @knitr components_plot
# canonical scores (components)
Z <- as.data.frame(X %*% U)
names(Z) <- c("Z1", "Z2")
Z$class <- y
# scatterplot (canonical variables)
ggplot(data = Z, aes(x = Z1, y = Z2, color = class)) +
geom_point()
plot(lda_fit)
rm(list = ls())
devtools::install_github("ucb-stat133/cointoss")
library(cointoss)
coin1 <- coin()
coin1
toss(coin1)
a = toss(coin1, 10)
a
unclass(a)
a
names(a)
a$tosses
summary(a)
?toss
shiny::runApp('Desktop/hackerwithin/histogram')
head(faithful)
runApp('Desktop/hackerwithin/histogram')
head(faithful)
runApp('Desktop/hackerwithin/histogram')
summary(mtcars[,1])
as.table(summary(mtcars[,1]))
as.matrix(summary(mtcars[,1]))
runApp('Desktop/hackerwithin/histogram')
head(faithful)
dim(faithful)
hist(faithful$waiting)
hist(faithful$waiting, col = 'gary70')
hist(faithful$waiting, col = 'gray70')
hist(faithful$waiting, col = 'gray90')
hist(faithful$waiting, col = 'gray90', las = 1)
hist(faithful$waiting, col = 'gray90', las = 1, xlab = 'x')
hist(faithful$waiting, col = 'gray90', breaks = 20)
runApp('Desktop/hackerwithin/histogram')
runApp('Desktop/hackerwithin/histogram')
runApp('Desktop/hackerwithin/histogram')
head(faithful)
runApp('Desktop/hackerwithin/histogram')
summary(faithful$waiting)
summary(faithful$waiting)
summary(faithful$eruptions)
runApp('Desktop/hackerwithin/histogram')
runApp('Dropbox/course_stat154/stat154-spring-2018/apps/roc-curve')
runApp('Dropbox/course_stat154/stat154-spring-2018/apps/roc-curve')
setwd("~/Dropbox/r4strings")
# read file
biomed <- read.table("data/biomedcentral.txt", header=TRUE, stringsAsFactors=FALSE)
utils::str(biomed, vec.len = 1)
head(biomed$Journal.name, n = 5)
titles10 <- head(biomed$Journal.name, n = 10)
titles10
# get first 10 names
titles10 <- head(biomed$Journal.name, n = 10)
titles10
# remove punctuation
titles10 <- str_replace_all(titles10, pattern = "[[:punct:]]", "")
library(stringr)
# read file
biomed <- read.table("data/biomedcentral.txt", header=TRUE, stringsAsFactors=FALSE)
# structure of the dataset
utils::str(biomed, vec.len = 1)
# first 5 journal names
head(biomed$Journal.name, n = 5)
# get first 10 names
titles10 <- head(biomed$Journal.name, n = 10)
titles10
# remove punctuation
titles10 <- str_replace_all(titles10, pattern = "[[:punct:]]", "")
titles10
# trim extra whitespaces
titles10 <- str_replace_all(titles10, pattern = "\\s+", " ")
titles10
# remove punctuation symbols
all_titles <- str_replace_all(biomed$Journal.name, pattern = "[[:punct:]]", "")
# trim extra whitespaces
all_titles <- str_replace_all(all_titles, pattern = "\\s+", " ")
# split titles by words
all_titles_list <- str_split(all_titles, pattern = " ")
# show first 2 elements
all_titles_list[1:2]
# how many words per title
words_per_title <- sapply(all_titles_list, length)
# table of frequencies
table(words_per_title)
# distribution
100 * round(table(words_per_title) / length(words_per_title), 4)
# summary
summary(words_per_title)
# longest journal
all_titles[which(words_per_title == 9)]
# vector of words in titles
title_words <- unlist(all_titles_list)
# get unique words
unique_words <- unique(title_words)
# how many unique words in total
num_unique_words <- length(unique(title_words))
num_unique_words
# vector to store counts
count_words <- rep(0, num_unique_words)
# count number of occurrences
for (i in 1:num_unique_words) {
count_words[i] <- sum(title_words == unique_words[i])
}
# table with word frequencies
count_words_alt <- table(title_words)
# table of frequencies
table(count_words)
# equivalently
table(count_words_alt)
# index values in decreasing order
top_30_order <- order(count_words, decreasing=TRUE)[1:30]
# top 30 frequencies
top_30_freqs <- sort(count_words, decreasing=TRUE)[1:30]
# select top 30 words
top_30_words <- unique_words[top_30_order]
top_30_words
op <- par(mar = c(6.5, 3, 2, 2))
# barplot
barplot(top_30_freqs,
border = NA,
names.arg = top_30_words,
las = 2,
ylim = c(0,100))
par(op)
library(stringr)
# read file
biomed <- read.table("data/biomedcentral.txt", header=TRUE, stringsAsFactors=FALSE)
# structure of the dataset
utils::str(biomed, vec.len = 1)
# first 5 journal names
head(biomed$Journal.name, n = 5)
# get first 10 names
titles10 <- head(biomed$Journal.name, n = 10)
titles10
# remove punctuation
titles10 <- str_replace_all(titles10, pattern = "[[:punct:]]", "")
titles10
# trim extra whitespaces
titles10 <- str_replace_all(titles10, pattern = "\\s+", " ")
titles10
# remove punctuation symbols
all_titles <- str_replace_all(biomed$Journal.name, pattern = "[[:punct:]]", "")
# trim extra whitespaces
all_titles <- str_replace_all(all_titles, pattern = "\\s+", " ")
all_titles
# split titles by words
all_titles_list <- str_split(all_titles, pattern = " ")
# show first 2 elements
all_titles_list[1:2]
# how many words per title
words_per_title <- sapply(all_titles_list, length)
# table of frequencies
table(words_per_title)
# distribution
100 * round(table(words_per_title) / length(words_per_title), 4)
# summary
summary(words_per_title)
# longest journal
all_titles[which(words_per_title == 9)]
# vector of words in titles
title_words <- unlist(all_titles_list)
# get unique words
unique_words <- unique(title_words)
# how many unique words in total
num_unique_words <- length(unique(title_words))
num_unique_words
